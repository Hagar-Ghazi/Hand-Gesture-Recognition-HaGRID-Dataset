<!-- # ğŸ– Hand Gesture Classification using MediaPipe Landmarks (HaGRID)

## Project Overview

This project aims to build a machine learning system capable of classifying hand gestures using landmark coordinates extracted from the **HaGRID (Hand Gesture Recognition Image Dataset)** via MediaPipe.

The input data consists of 3D hand landmark coordinates (x, y, z) representing 21 keypoints per detected hand.  
The output is a trained classification model that predicts the corresponding gesture class.
This project follows a structured ML workflow including data preprocessing, model training, evaluation and experiment tracking using MLflow.

---

## Dataset Description

- Dataset: HaGRID (Hand Gesture Recognition Image Dataset)
- 21 hand landmarks per sample
- Each landmark contains:
  - x coordinate
  - y coordinate
  - z coordinate
- Total numerical features: **63**
- Number of gesture classes: **18**

### Preprocessing Strategy

To ensure scale and position invariance:

- Recenter all (x, y) coordinates so the wrist landmark becomes the origin.
- Normalize (x, y) coordinates by dividing by the middle finger tip position.
- The z-coordinate is left unchanged as it is already normalized by MediaPipe.

---

##  Project Structure

```
Hand-Gesture-Recognition-HaGRID-Dataset/

Hand-Gesture-Recognition-HaGRID-Dataset/
â”œâ”€â”€ Data/                   # Dataset files
â”œâ”€â”€ Notebooks/              # Source code: Visualization, Preprocessing, Models, Evaluation
â”œâ”€â”€ MLflow Screenshots/     # MLflow UI screenshots (research experiments)
â”œâ”€â”€ mlruns/                 # MLflow experiment tracking (research branch)
â”œâ”€â”€ MLflow Notebooks/       # Notebooks related to MLflow experiments
â”œâ”€â”€ README.md               # This file
â””â”€â”€ Requirements.txt        # Project dependencies

```

## ğŸ¤– Machine Learning Models

At least three classification models will be implemented and compared including:

- Random Forest
- Support Vector Machine (SVM)
- Logistic Regression
- (Optional) Gradient Boosting / XGBoost

Hyperparameter tuning and performance comparison will be conducted using MLflow experiment tracking.

---

## ğŸ“Š Evaluation Metrics

Model performance will be evaluated using:

- Accuracy
- Precision
- Recall
- F1-Score
- Confusion Matrix

The best-performing model will be selected based on overall generalization performance.

---

## Experiment Tracking

All experiments, parameters, metrics and models will be tracked using MLflow (research branch only).

The final selected model will be registered in the MLflow Model Registry.

---

## Deployment Demonstration

A demonstration video will show:

- Real-time hand landmark extraction using MediaPipe
- Gesture prediction per frame
- Stabilized output using a sliding window mode technique

---

##  Repository Branch Strategy

- `main` branch â†’ Clean ML1 project deliverables (no MLflow code)
- `research` branch â†’ Full MLflow integration and experiment tracking

---

## Status

Project initialization phase â€” development in progress -->



# ğŸ– Hand Gesture Classification using MediaPipe Landmarks (HaGRID)

## Overview

This project implements a complete **endâ€‘toâ€‘end machine learning pipeline** for static hand gesture recognition using 3D landmark coordinates extracted with MediaPipe from the **HaGRID (Hand Gesture Recognition Image Dataset)**

The system learns discriminative geometric patterns of hand poses and outputs a trained classifier capable of **realâ€‘time gesture prediction** from webcam streams


![alt text](<.gitignore/images/Class Distribution.png>)


The project follows a reproducible ML lifecycle:

1. Data acquisition & inspection
2. Landmark visualization
3. Geometric normalization
4. Feature preparation
5. Model training & tuning
6. Evaluation & error analysis
7. Experiment tracking (MLflow)
8. Realâ€‘time inference demo

---

##  Objectives

* Build a robust gesture classifier invariant to hand position & scale
* Compare multiple classical ML algorithms on landmark geometry
* Analyze perâ€‘gesture performance and confusion patterns
* Demonstrate realâ€‘time prediction using MediaPipe landmarks
* Ensure reproducibility via MLflow tracking

---

## ğŸ§  Problem Formulation

Each detected hand is represented by **21 anatomical landmarks** with 3D coordinates:

* x â†’ horizontal location
* y â†’ vertical location
* z â†’ relative depth (MediaPipe normalized)

Feature vector size:

```
21 landmarks Ã— 3 coordinates = 63 features
```

Target:

```
Gesture label âˆˆ {18 classes}
```
![alt text](<.gitignore/images/Hand Landmarks.png>)

---

## ğŸ“‚ Dataset â€” HaGRID

HaGRID is a largeâ€‘scale hand gesture dataset containing diverse hand poses captured in realâ€‘world conditions

Dataset characteristics:

* 18 gesture categories
* Pose orientation variation
* Scale variation
* Background diversity
* Different lighting conditions
* Multiple subjects

Landmarks are extracted using **MediaPipe Hands** producing consistent anatomical keypoints

![alt text](<.gitignore/images/HaGRID Dataset.png>)

---

## ğŸ” Landmark Representation

MediaPipe provides 21 standardized hand keypoints:

* Wrist
* Thumb (4 joints)
* Index (4 joints)
* Middle (4 joints)
* Ring (4 joints)
* Pinky (4 joints)

This representation encodes full hand articulation and finger configuration, enabling gesture discrimination purely from geometry

![alt text](<.gitignore/images/hand landmarks .png>)
---

## âš™ï¸ Preprocessing & Normalization Pipeline

Raw landmarks depend on camera position and hand scale. To achieve invariance geometric normalization is applied.

### 1ï¸âƒ£ Translation Normalization (Reâ€‘centering)

Wrist landmark becomes origin:

```
x' = x âˆ’ x_wrist
y' = y âˆ’ y_wrist
```

Effect:

* Removes global hand position
* Aligns all hands to same reference point

---

### 2ï¸âƒ£ Scale Normalization

Coordinates divided by distance to middleâ€‘finger tip:

```
d = âˆš((x_mid âˆ’ x_wrist)Â² + (y_mid âˆ’ y_wrist)Â²)
x'' = x' / d
y'' = y' / d
```

Effect:

* Removes hand size variation
* Normalizes across users & camera distance

---

### 3ï¸âƒ£ Depth Handling

MediaPipe z is already normalized â†’ kept unchanged.

Result:

**Poseâ€‘dependent but position & scale invariant landmark geometry**


![alt text](<.gitignore/images/normalized handlandmark.png>)
---

## ğŸ“Š Exploratory Data Analysis (EDA)

The notebook includes extensive visualization to understand dataset structure:

### Analyses Performed

* Class distribution histogram
* Gesture imbalance analysis
* Landmark coordinate scatter plots
* Normalized hand skeleton visualization
* Sample geometry per class

### Key Findings

* Dataset shows class imbalance across gestures
* Normalization aligns hand geometry across samples
* Distinct spatial patterns exist per gesture
* Landmark clusters become separable postâ€‘normalization

---

## ğŸ§¾ Feature Preparation

After normalization:

* Features flattened into 63â€‘D vector
* Optional scaling (StandardScaler)
* Train/validation split
* Stratified sampling to preserve class distribution

Feature characteristics:

* Continuous numeric
* Low dimensional (63)
* Highly correlated within fingers
* Nonlinear interâ€‘finger relationships

---

## ğŸ¤– Machine Learning Models

Multiple classical classifiers were trained and compared.

### Logistic Regression

* Linear decision boundary
* Baseline geometric separability

### Support Vector Machine (SVM)

* RBF kernel
* Captures nonlinear pose relations
* Strong margin separation

### Random Forest

* Nonlinear tree ensemble
* Handles feature interactions
* Robust to noise

### (Optional) Gradient Boosting / XGBoost

* Sequential tree boosting
* Fine boundary modeling

---

## ğŸ”§ Hyperparameter Tuning

Examples of tuned parameters:

* SVM: C, gamma, kernel
* Random Forest: n_estimators, max_depth, min_samples_split
* Logistic Regression: C, penalty

Search strategy:

* Grid search / manual sweep
* Validation score comparison
* MLflow tracking

---

## ğŸ“ˆ Evaluation Metrics

Model performance evaluated using multiple complementary metrics

### Primary Metrics

* Accuracy
* Precision (macro & weighted)
* Recall (macro & weighted)
* F1â€‘Score (macro & weighted)
* Confusion Matrix

---

##  Quantitative Performance Comparison

The following table summarizes the performance of the primary models evaluated in this study
Metrics were calculated on the test set (5,135 samples) using a weighted average to ensure a balanced assessment


| Model               | Accuracy|F1-Score  | precision | Recall | Status        |
|---------------------|---------|----------|-----------|--------|---------------|
| XGBoost             | 0.9834  | 0.9835   | 0.9836    | 0.9834 | Challenger    |
| Random Forest       | 0.9778  | 0.9779   | 0.9781    | 0.9778 | Baseline      |
| SVM (Tuned)         | 0.9894  | 0.9894   | 0.9895    | 0.9894 | Champion      |
| Logistic Regression | 0.8555  | 0.8551   | 0.8574    | 0.8555 | Baseline      |


---


### Why Multiple Metrics?

Gesture datasets are often imbalanced so accuracy alone is misleading
Macroâ€‘F1 ensures equal importance across gestures.

---

## ğŸ“Š Error Analysis

Confusion matrix analysis reveals:

* Similar finger configurations causing confusion
* Visually close gestures overlapping in feature space
* Rare classes with lower recall

Insights used to guide:

* Model choice
* Feature normalization
* Future augmentation strategies

### Confusion Matrix Analysis for each model

![alt text](.gitignore/images/cm1.png)



![alt text](.gitignore/images/cm2.png)

---

---

## ğŸ§ª Experiment Tracking (MLflow)

The research branch integrates MLflow for reproducibility.

Tracked per run:

* Model type
* Hyperparameters
* Training metrics
* Validation metrics
* Confusion matrix image
* Trained model artifact

Benefits:

* Comparable experiments
* Reproducibility
* Bestâ€‘model selection
* Registry deployment

![alt text](.gitignore/images/Runs.png)

![alt text](.gitignore/images/Runs.png)

---


## ğŸ¥ Realâ€‘Time Inference Pipeline

The deployed demo performs perâ€‘frame gesture recognition.

### Steps

1. Capture video
2. Detect hand via MediaPipe
3. Extract 21 landmarks
4. Apply normalization
5. Predict gesture
6. Display stabilized label


![alt text](.gitignore/images/demo.gif)

### Temporal Stabilization

Slidingâ€‘window majority voting reduces jitter and mispredictions across frames.

---

##  Repository Structure

```
Hand-Gesture-Recognition-HaGRID-Dataset/
â”‚
â”œâ”€â”€ Data/                   # Dataset & landmark files
â”œâ”€â”€ Notebooks/              # EDA, preprocessing, training, evaluation
â”œâ”€â”€ MLflow Notebooks/       # Experiment tracking workflows
â”œâ”€â”€ MLflow Screenshots/     # MLflow UI results
â”œâ”€â”€ mlruns/                 # MLflow artifacts
â”œâ”€â”€ Requirements.txt        # Dependencies
â””â”€â”€ README.md               # Documentation


```


## ğŸ† Results Summary

Key outcomes observed in experiments:

* Normalization significantly improved separability
* Nonlinear models outperformed linear baseline
* SVM / XGBoost achieved highest F1
* Stable realâ€‘time predictions achieved with smoothing

![alt text](.gitignore/images/charts2.png)
---

## ğŸš€ Future Improvements

* Deep learning on landmark sequences (LSTM / Transformer)
* Dynamic gesture recognition
* Multiâ€‘hand support
* Mobile deployment
* Dataset balancing / augmentation

---

## ğŸ§¾ Requirements

Core libraries:

* Python 3.10+
* MediaPipe
* NumPy
* Pandas
* Scikitâ€‘learn
* Matplotlib / Seaborn
* MLflow
* OpenCV

Install:

```
pip install -r Requirements.txt
```

